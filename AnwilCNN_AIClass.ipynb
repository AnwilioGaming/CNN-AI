{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C8cMqR64GDVk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import  torch.nn.functional as F\n",
        "from torchvision import transforms, datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform =  transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ],
      "metadata": {
        "id": "Z_fnut0JGRIk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasets = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "test_datasets = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "RPLkn9kCGZXQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_datasets, batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_datasets, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "bzAe4moPGZ4-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "   def __init__(self):\n",
        "     super(SimpleCNN, self).__init__()\n",
        "     self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "     self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "     self.pool = nn.MaxPool2d(2,2)\n",
        "     self.fc1 = nn.Linear(9216, 128)  # Changed from 3136 to 9216\n",
        "     self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "   def forward(self, x):\n",
        "     x = F.relu(self.conv1(x))\n",
        "     x = self.pool(F.relu(self.conv2(x)))\n",
        "     x = x.view(-1, 9216)  # Changed from 3136 to 9216\n",
        "     x = F.relu(self.fc1(x))\n",
        "     x = self.fc2(x)\n",
        "     return x\n",
        "\n",
        "model = SimpleCNN()"
      ],
      "metadata": {
        "id": "bYD59_c8Gc7T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "XuaBX5joGgFe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "     optimizer.zero_grad()\n",
        "     output = model(data)\n",
        "     loss = criterion(output, target)\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "     if batch_idx % 100 == 0:\n",
        "        print('Train Epoch: {} [{}/{}] Loss: {:.6f}'.format( epoch, batch_idx * len(data) , len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkVA97bRGkJS",
        "outputId": "89ed11ee-6b33-41b2-ff3c-a74314e7a6e3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/938] Loss: 2.297052\n",
            "Train Epoch: 0 [6400/938] Loss: 0.237167\n",
            "Train Epoch: 0 [12800/938] Loss: 0.160893\n",
            "Train Epoch: 0 [19200/938] Loss: 0.069632\n",
            "Train Epoch: 0 [25600/938] Loss: 0.120204\n",
            "Train Epoch: 0 [32000/938] Loss: 0.054922\n",
            "Train Epoch: 0 [38400/938] Loss: 0.137305\n",
            "Train Epoch: 0 [44800/938] Loss: 0.178326\n",
            "Train Epoch: 0 [51200/938] Loss: 0.144495\n",
            "Train Epoch: 0 [57600/938] Loss: 0.102069\n",
            "Train Epoch: 1 [0/938] Loss: 0.115758\n",
            "Train Epoch: 1 [6400/938] Loss: 0.041028\n",
            "Train Epoch: 1 [12800/938] Loss: 0.019875\n",
            "Train Epoch: 1 [19200/938] Loss: 0.154026\n",
            "Train Epoch: 1 [25600/938] Loss: 0.053251\n",
            "Train Epoch: 1 [32000/938] Loss: 0.002642\n",
            "Train Epoch: 1 [38400/938] Loss: 0.113240\n",
            "Train Epoch: 1 [44800/938] Loss: 0.104249\n",
            "Train Epoch: 1 [51200/938] Loss: 0.002880\n",
            "Train Epoch: 1 [57600/938] Loss: 0.035903\n",
            "Train Epoch: 2 [0/938] Loss: 0.008060\n",
            "Train Epoch: 2 [6400/938] Loss: 0.011591\n",
            "Train Epoch: 2 [12800/938] Loss: 0.003835\n",
            "Train Epoch: 2 [19200/938] Loss: 0.001307\n",
            "Train Epoch: 2 [25600/938] Loss: 0.063053\n",
            "Train Epoch: 2 [32000/938] Loss: 0.009204\n",
            "Train Epoch: 2 [38400/938] Loss: 0.010449\n",
            "Train Epoch: 2 [44800/938] Loss: 0.005034\n",
            "Train Epoch: 2 [51200/938] Loss: 0.021390\n",
            "Train Epoch: 2 [57600/938] Loss: 0.054745\n",
            "Train Epoch: 3 [0/938] Loss: 0.020538\n",
            "Train Epoch: 3 [6400/938] Loss: 0.001027\n",
            "Train Epoch: 3 [12800/938] Loss: 0.001486\n",
            "Train Epoch: 3 [19200/938] Loss: 0.005575\n",
            "Train Epoch: 3 [25600/938] Loss: 0.051128\n",
            "Train Epoch: 3 [32000/938] Loss: 0.002235\n",
            "Train Epoch: 3 [38400/938] Loss: 0.044914\n",
            "Train Epoch: 3 [44800/938] Loss: 0.005368\n",
            "Train Epoch: 3 [51200/938] Loss: 0.020148\n",
            "Train Epoch: 3 [57600/938] Loss: 0.002306\n",
            "Train Epoch: 4 [0/938] Loss: 0.113839\n",
            "Train Epoch: 4 [6400/938] Loss: 0.004271\n",
            "Train Epoch: 4 [12800/938] Loss: 0.055740\n",
            "Train Epoch: 4 [19200/938] Loss: 0.003226\n",
            "Train Epoch: 4 [25600/938] Loss: 0.049747\n",
            "Train Epoch: 4 [32000/938] Loss: 0.000681\n",
            "Train Epoch: 4 [38400/938] Loss: 0.000600\n",
            "Train Epoch: 4 [44800/938] Loss: 0.001398\n",
            "Train Epoch: 4 [51200/938] Loss: 0.000901\n",
            "Train Epoch: 4 [57600/938] Loss: 0.000501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "   for data, target in test_loader:\n",
        "      output = model(data)\n",
        "      test_loss += criterion(output, target).item()\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      test_loss /= len(test_loader.dataset)\n",
        "      accuracy = 100. * correct / len(test_loader.dataset)\n",
        "      print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(test_loss, correct, len(test_loader.dataset), accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hcpfq2UGm0d",
        "outputId": "a515825b-8d3f-4e9e-f7eb-aa6f5176f3ae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0000, Accuracy: 64/10000 (1%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 128/10000 (1%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 192/10000 (2%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 256/10000 (3%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 320/10000 (3%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 382/10000 (4%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 445/10000 (4%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 509/10000 (5%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 573/10000 (6%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 636/10000 (6%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 699/10000 (7%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 762/10000 (8%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 826/10000 (8%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 888/10000 (9%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 950/10000 (10%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1013/10000 (10%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1076/10000 (11%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1140/10000 (11%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1202/10000 (12%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1262/10000 (13%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1326/10000 (13%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1390/10000 (14%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1453/10000 (15%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1517/10000 (15%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1581/10000 (16%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1644/10000 (16%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1708/10000 (17%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1771/10000 (18%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1835/10000 (18%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1897/10000 (19%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 1961/10000 (20%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2023/10000 (20%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2087/10000 (21%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2147/10000 (21%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2210/10000 (22%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2273/10000 (23%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2337/10000 (23%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2399/10000 (24%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2461/10000 (25%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2525/10000 (25%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2588/10000 (26%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2651/10000 (27%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2715/10000 (27%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2778/10000 (28%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2842/10000 (28%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2903/10000 (29%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 2967/10000 (30%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3030/10000 (30%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3093/10000 (31%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3157/10000 (32%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3220/10000 (32%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3284/10000 (33%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3348/10000 (33%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3410/10000 (34%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3474/10000 (35%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3535/10000 (35%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3599/10000 (36%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3662/10000 (37%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3723/10000 (37%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3785/10000 (38%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3848/10000 (38%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3910/10000 (39%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 3972/10000 (40%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4034/10000 (40%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4098/10000 (41%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4160/10000 (42%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4222/10000 (42%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4285/10000 (43%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4349/10000 (43%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4413/10000 (44%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4474/10000 (45%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4536/10000 (45%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4599/10000 (46%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4662/10000 (47%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4724/10000 (47%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4787/10000 (48%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4851/10000 (49%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4915/10000 (49%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 4979/10000 (50%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5043/10000 (50%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5107/10000 (51%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5170/10000 (52%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5234/10000 (52%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5298/10000 (53%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5362/10000 (54%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5426/10000 (54%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5490/10000 (55%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5554/10000 (56%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5617/10000 (56%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5680/10000 (57%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5744/10000 (57%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5808/10000 (58%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5871/10000 (59%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5932/10000 (59%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 5996/10000 (60%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6060/10000 (61%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6123/10000 (61%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6187/10000 (62%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6251/10000 (63%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6315/10000 (63%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6379/10000 (64%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6443/10000 (64%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6506/10000 (65%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6567/10000 (66%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6631/10000 (66%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6695/10000 (67%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6759/10000 (68%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6823/10000 (68%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6887/10000 (69%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 6951/10000 (70%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7015/10000 (70%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7079/10000 (71%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7143/10000 (71%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7207/10000 (72%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7271/10000 (73%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7335/10000 (73%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7399/10000 (74%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7463/10000 (75%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7527/10000 (75%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7591/10000 (76%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7655/10000 (77%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7718/10000 (77%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7782/10000 (78%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7845/10000 (78%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7909/10000 (79%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 7973/10000 (80%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8036/10000 (80%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8100/10000 (81%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8164/10000 (82%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8227/10000 (82%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8290/10000 (83%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8353/10000 (84%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8417/10000 (84%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8481/10000 (85%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8545/10000 (85%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8609/10000 (86%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8673/10000 (87%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8737/10000 (87%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8801/10000 (88%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8865/10000 (89%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8926/10000 (89%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 8988/10000 (90%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9052/10000 (91%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9116/10000 (91%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9180/10000 (92%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9244/10000 (92%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9308/10000 (93%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9372/10000 (94%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9435/10000 (94%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9498/10000 (95%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9559/10000 (96%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9619/10000 (96%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9682/10000 (97%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9745/10000 (97%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9809/10000 (98%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9873/10000 (99%)\n",
            "Test set: Average loss: 0.0000, Accuracy: 9889/10000 (99%)\n"
          ]
        }
      ]
    }
  ]
}